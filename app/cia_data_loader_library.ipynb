{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0ef292-8166-4467-ab79-60661e0e5d5d",
   "metadata": {},
   "source": [
    "# Funciones desarrolladas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d8951-5c26-4106-8132-56a676e79bee",
   "metadata": {},
   "source": [
    "1.2. üõ†Ô∏è Instalaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e3887-b9c9-451c-b04d-2fdbab4427c8",
   "metadata": {},
   "source": [
    "### üîßCarga de librerias en el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15710195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de CSV\n",
    "import pandas as pd\n",
    "\n",
    "# Funciones de carga de ficheros\n",
    "import os\n",
    "# Para generar el gr√°fico de dependecias\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Calculo de localizaciones\n",
    "import math\n",
    "import geocoder\n",
    "\n",
    "import inspect\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51439849-9cc0-483a-bb66-b340b8c70f4b",
   "metadata": {},
   "source": [
    "## üíª Funciones sobre el sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc8300-980b-4581-9676-a5f1475c27a2",
   "metadata": {},
   "source": [
    "### clean_file\n",
    "\n",
    "üîç ¬øQu√© hace?\n",
    "\n",
    "Esta funci√≥n recorre todos los archivos dentro de un **directorio (y sus subdirectorios)** y limpia los saltos de l√≠nea Windows (**\\r\\n**), reemplaz√°ndolos por saltos de l√≠nea est√°ndar de Unix (\\n), en archivos que tengan extensiones espec√≠ficas (*por defecto, .txt*).\n",
    "\n",
    "üß± ¬øQu√© hace paso a paso?\n",
    "\n",
    "Recorrer el directorio y subdirectorios. \n",
    "\n",
    "La funci√≥n filtra los archivos para solo procesar aquellos que tienen extensiones que coinciden con las proporcionadas en el par√°metro extensions.\n",
    "\n",
    "Abrir y leer el contenido del archivo:\n",
    "\n",
    "Limpiar el contenido del archivo reemplaza los saltos de l√≠nea \\r\\n (utilizados en sistemas Windows) por \\n (salto de l√≠nea est√°ndar en sistemas Unix/Linux).\n",
    "\n",
    "\n",
    "Despu√©s de limpiar el contenido y se sobrescribe con el contenido limpio.\n",
    "\n",
    "Manejo de errores:\n",
    "\n",
    "Si ocurre un error durante la lectura o escritura del archivo, se captura con un bloque try-except y se imprime un mensaje de error, pero el proceso contin√∫a con el siguiente archivo.\n",
    "\n",
    "Si ocurre un error al recorrer los directorios o abrir el archivo principal, se devuelve un mensaje de error indicando la excepci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8892690f-f89f-4458-81af-2664ee6e1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(directory, extensions=['.txt']):\n",
    "    try : \n",
    "      for root, dirs, files in os.walk(directory):  # Recorrer directorios y subdirectorios\n",
    "        for file_name in files:\n",
    "            # print('extension:',extensions)\n",
    "            if any(file_name.endswith(extension) for extension in extensions):  # Filtra por extensiones\n",
    "                file_path = os.path.join(root, file_name)  # Obtiene la ruta completa\n",
    "                # print(f'Procesando archivo: {file_path}')\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "\n",
    "                    content_cleaned = content.replace('\\r\\n', '\\n')\n",
    "\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(content_cleaned)\n",
    "\n",
    "                    # print(f'‚úî Procesado: {file_name}')\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f'‚ùå Error al procesar {file_name}: {e}. Se omitir√°.')\n",
    "    except Exception:\n",
    "        return f'error {Exception}'\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7e02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a221ab-2534-46ca-bdda-2f4d84c273c5",
   "metadata": {},
   "source": [
    "## Importaci√≥n de Librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e049a-7438-4cf0-b20b-afdd03f3f01c",
   "metadata": {},
   "source": [
    "## üìä De tratamiento de las entidades del dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d0c42-15a7-4689-97f7-9975803735bb",
   "metadata": {},
   "source": [
    "### lowercase_if_letters_only\n",
    "\n",
    "üîç ¬øQu√© hace?\n",
    "\n",
    "Funci√≥n para convertir a min√∫sculas solo las columnas con letras\n",
    "\n",
    "\n",
    "üß± ¬øQu√© hace paso a paso?\n",
    "\n",
    "    1. Recorre todas las columnas del DataFrame.\n",
    "    2. Para cada columna:\n",
    "       - Convierte cada valor a cadena.\n",
    "       - Verifica si contiene solo letras. (para evitar tocar las columnas que tienen numeros\n",
    "    3. Si todos los valores de la columna son letras:\n",
    "       - Convierte la columna completa a min√∫sculas usando.\n",
    "    4. Devuelve el DataFrame modificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c6aee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_if_letters_only(df_temp):\n",
    "    try: \t\n",
    "        for col in df_temp.columns:\n",
    "            if df_temp[col].apply(lambda x: str(x).isalpha()).all():\n",
    "                df_temp[col] = df_temp[col].str.lower()\n",
    "        return df_temp\n",
    "    except Exception:\n",
    "        return f'error {Exception}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab854f-9c5c-4578-b38a-2ecf7bf67178",
   "metadata": {},
   "source": [
    "### is_words(column)\n",
    "\n",
    " üîç ¬øQu√© hace?\n",
    "\n",
    " Verifica si todos los valores de una columna son solo letras (sin n√∫meros ni caracteres especiales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911aeb7e-bd48-4bf0-b386-f012061e2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_words(column):\n",
    "   return column.apply(lambda x: bool(re.match(r'^[A-Za-z]+$', str(x)))).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642ffae-0c33-48a8-8122-68dd5dc44abf",
   "metadata": {},
   "source": [
    "### lower_column\n",
    "\n",
    "üîç ¬øQu√© hace?\n",
    "\n",
    "Verifica si todos los valores en una serie contienen solo letras (ignorando strings vac√≠os).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001d033d-838c-407c-81b6-52dea29f3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_column(df_temp, columns_str):\n",
    "    for column in columns_str:\n",
    "        if column in df_temp.columns:\n",
    "            # Convertir la columna a tipo string, manejando nulos\n",
    "            df_temp[column] = df_temp[column].fillna('').astype(str)\n",
    "            \n",
    "            # Verificar si todos los valores son letras\n",
    "            if is_words(df_temp[column]):\n",
    "                # Si todos los valores son letras, convertir a min√∫sculas\n",
    "                df_temp[column] = df_temp[column].str.lower()\n",
    "\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fb84c-94c9-4159-a94a-84fb102848a5",
   "metadata": {},
   "source": [
    "### load_files_in_dataframes\n",
    "\n",
    "üîç ¬øQu√© hace?\n",
    "\n",
    "La funci√≥n load_files_in_dataframes carga todos los archivos .csv o .txt de un directorio (y sus subdirectorios) en DataFrames de pandas, realizando varias transformaciones espec√≠ficas.\n",
    "\n",
    "üß± Paso a paso\n",
    "* Inicializa listas y diccionarios:\n",
    "\n",
    "lista_id: almacena nombres de columnas que terminan en _id o empiezan por id.\n",
    "\n",
    "dataframes: contiene los DataFrames con claves basadas en el nombre del archivo.\n",
    "\n",
    "üîπ  Recorre los archivos del directorio:\n",
    "\n",
    "* Ignora subdirectorios (aunque los recorre recursivamente).\n",
    "\n",
    "üîπ Filtra archivos por extensi√≥n (.csv o .txt).\n",
    "\n",
    "* Procesa cada archivo v√°lido:\n",
    "\n",
    "* Lee el archivo como DataFrame con pandas.read_csv().\n",
    "\n",
    "* Convierte los nombres de columnas a min√∫sculas.\n",
    "\n",
    "* Renombra columnas usando convert_list.\n",
    "\n",
    "* Realiza slicing o manipulaci√≥n de columnas definidas en convert_keys.\n",
    "\n",
    "* Si existe la columna objectid, la usa como √≠ndice.\n",
    "\n",
    "üîπ Limpieza de datos:\n",
    "\n",
    "* Detecta columnas que contengan solo letras (excepto las columnas con id) y las convierte a min√∫sculas con lower_column.\n",
    "\n",
    "üîπ Guarda el resultado:\n",
    "\n",
    "* Crea un DataFrame con nombre df_nombre_del_archivo.\n",
    "\n",
    "* Tambi√©n lo guarda en el diccionario dataframes.\n",
    "\n",
    "üîπ Manejo de errores:\n",
    "\n",
    "* Ignora archivos vac√≠os o con errores de an√°lisis (pandas.errors.EmptyDataError o ParserError).\n",
    "\n",
    "* Captura otros errores inesperados sin interrumpir el procesamiento.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3397b662-20af-4b20-a098-ea3db238d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_files_in_dataframes(file_path, sep=',', extensions=['.txt', '.csv']):\n",
    "    all_ids = set()\n",
    "    dataframes = {}\n",
    "\n",
    "    try:\n",
    "        for full_file_name in os.listdir(file_path):\n",
    "            filepath = os.path.join(file_path, full_file_name)\n",
    "            \n",
    "            # Si es una subcarpeta, recursivamente procesa\n",
    "            if os.path.isdir(filepath):\n",
    "                sub_dataframes, sub_ids = load_files_in_dataframes(filepath, sep=sep, extensions=extensions)\n",
    "                dataframes.update(sub_dataframes)\n",
    "                all_ids.update(sub_ids)\n",
    "                continue\n",
    "\n",
    "            # Procesar solo archivos con extensi√≥n v√°lida\n",
    "            if any(full_file_name.endswith(ext) for ext in extensions):\n",
    "                file_name = os.path.splitext(full_file_name)[0]  # sin extensi√≥n\n",
    "                print(f'Procesando archivo: {full_file_name}')\n",
    "                try:\n",
    "                    df = pd.read_csv(filepath, sep=sep, header=0)\n",
    "                    df.columns = df.columns.str.lower()  # Normalizar nombres de columnas\n",
    "                    \n",
    "                    # Detectar columnas ID\n",
    "                    ids_in_file = [col for col in df.columns if col.endswith('_id') or col.startswith('id')]\n",
    "                    all_ids.update(ids_in_file)\n",
    "                    \n",
    "                    if ids_in_file:\n",
    "                        print(f'df_{file_name} contiene IDs: {ids_in_file}')\n",
    "\n",
    "                    df_name = f\"df_{file_name}\"\n",
    "                    globals()[df_name] = df  # no recomendable fuera de notebooks, pero √∫til en exploraci√≥n\n",
    "                    dataframes[file_name] = df\n",
    "\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    print(f\"Advertencia: El archivo {full_file_name} est√° vac√≠o. Se omitir√°.\")\n",
    "                except pd.errors.ParserError as e:\n",
    "                    print(f\"Error al analizar el archivo {full_file_name}: {e}. Se omitir√°.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inesperado al procesar {full_file_name}: {e}. Se omitir√°.\")\n",
    "\n",
    "        return dataframes, list(all_ids)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El directorio '{file_path}' no existe.\")\n",
    "        return {}, []\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado: {e}\")\n",
    "        return {}, []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c1449-a46e-4a31-ad12-989cfff86934",
   "metadata": {},
   "source": [
    "### detect_keys\n",
    "\n",
    "üîç ¬øQue hace?\n",
    "\n",
    "üîπ Detectar autom√°ticamente claves primarias y claves for√°neas en un DataFrame de pandas, bas√°ndose en los nombres de columnas y si sus valores son √∫nicos o repetidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8201ca7-3aa0-4326-90cc-fd8df93a4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_keys(df):\n",
    "    # Detectar claves primarias (columnas √∫nicas que terminan con '_id' o comienzan con 'id')\n",
    "    primary_keys = [col for col in df.columns if df[col].is_unique and (col.endswith('_id') or col.startswith('id'))]\n",
    "    \n",
    "    # Detectar claves for√°neas (columnas no √∫nicas que terminan con '_id')\n",
    "    foreign_keys = [col for col in df.columns if not df[col].is_unique and col.endswith('_id')]\n",
    "    \n",
    "    return primary_keys, foreign_keys "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca898dae-a994-41df-99c0-a2b2ea4e67d1",
   "metadata": {},
   "source": [
    "### build_dependency_graph\n",
    "\n",
    "üîç ¬øQue hace?\n",
    "\n",
    "La funci√≥n construye un grafo de dependencias, entre tablas a partir de un conjunto de DataFrames de pandas. Este grafo permite conocer en qu√© orden se deben insertar las tablas en una base de datos relacional (como PostgreSQL), respetando sus dependencias por claves for√°neas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e28c64a-eb48-4422-9791-15c83efcb2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dependency_graph(dataframes):\n",
    "    graph = defaultdict(list)  # Diccionario para almacenar las dependencias\n",
    "    all_tables = set(dataframes.keys())  # Conjunto de todos los nombres de tablas\n",
    "\n",
    "    for table_name, df in dataframes.items():\n",
    "        # Detecta claves primarias y for√°neas en el DataFrame\n",
    "        _, foreign_keys = detect_keys(df)\n",
    "        \n",
    "        for fk in foreign_keys:\n",
    "            # Elimina el sufijo '_id' para obtener el nombre de la tabla referenciada\n",
    "            ref_table = fk[:-3] if fk.endswith('_id') else fk\n",
    "            \n",
    "            # Si la tabla referenciada existe en el conjunto de tablas, agrega la dependencia\n",
    "            if ref_table in dataframes:\n",
    "                graph[ref_table].append(table_name)\n",
    "        \n",
    "        # Asegura que la tabla actual est√© en el grafo, incluso si no tiene dependencias\n",
    "        if table_name not in graph:\n",
    "            graph[table_name] = []\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d36df-c113-4ff5-b608-53e1996558db",
   "metadata": {},
   "source": [
    "### topological_sort_all_nodes\n",
    "\n",
    "üîç ¬øQue hace?\n",
    "\n",
    "Su objetivo es determinar un orden de inserci√≥n de tablas en una base de datos respetando las dependencias entre ellas (por ejemplo, claves for√°neas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b4340f-51d7-4d58-9272-fb8b017b9636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort_all_nodes(graph):\n",
    "    indegree = {node: 0 for node in graph}\n",
    "    for node in graph:\n",
    "        for neighbor in graph[node]:\n",
    "            indegree[neighbor] += 1\n",
    "\n",
    "    queue = deque([node for node in graph if indegree[node] == 0])\n",
    "    order = []\n",
    "\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        order.append(node)\n",
    "        for neighbor in graph[node]:\n",
    "            indegree[neighbor] -= 1\n",
    "            if indegree[neighbor] == 0:\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    if len(order) != len(graph):\n",
    "        print(\"‚ö†Ô∏è Cuidado: se detect√≥ una posible dependencia c√≠clica.\")\n",
    "    return order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f59fc7fd-327a-40fa-b247-8f40e0f8ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(graph):\n",
    "    visited = set()\n",
    "    order = []\n",
    "\n",
    "    def dfs(node):\n",
    "        if node in visited:\n",
    "            return\n",
    "        visited.add(node)\n",
    "        for neighbor in graph.get(node, []):\n",
    "            dfs(neighbor)\n",
    "        order.append(node)\n",
    "\n",
    "    for node in graph:\n",
    "        dfs(node)\n",
    "\n",
    "    return list(reversed(order))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b486816e-7daf-459d-80af-e9c4af7869c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_columns(df):\n",
    "    \"\"\"\n",
    "    Retorna un diccionario con el nombre de la variable del DataFrame y sus columnas con valores √∫nicos.\n",
    "    \"\"\"\n",
    "    # Inspeccionar el stack para encontrar el nombre de la variable pasada como argumento\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    df_name = next((name for name, val in callers_local_vars if val is df), \"dataframe\")\n",
    "\n",
    "    unique_cols = [col for col in df.columns if df[col].is_unique]\n",
    "    return df_name,unique_cols\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d4a91-09ad-43f7-a98d-780a70116062",
   "metadata": {},
   "source": [
    "### insert_dataframes_to_postgres\n",
    "\n",
    "\n",
    "üîç ¬øQu√© hace?\n",
    "Inserta DataFrames en una base de datos PostgreSQL en el orden especificado, creando las tablas si no existen.\n",
    "\n",
    "    Par√°metros:\n",
    "        - dataframes: dict de DataFrames a insertar.\n",
    "        - insertion_order: orden de inserci√≥n (respetando claves for√°neas).\n",
    "        - postgres_user: usuario de la base de datos.\n",
    "        - postgres_password: contrase√±a.\n",
    "        - postgres_db: nombre de la base de datos.\n",
    "        - host: direcci√≥n del servidor PostgreSQL.\n",
    "        - port: puerto del servidor.\n",
    "        - if_exists: comportamiento si la tabla existe ('fail', 'replace', 'append').\n",
    "        - unique_columns: Un diccionario que mapea el nombre de la tabla con las columnas que se deben considerar √∫nicas para evitar duplicados. Esto es √∫til cuando queremos evitar que se inserten registros con valores duplicados en esas columnas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "üß± Paso a paso\n",
    "\n",
    "Conexi√≥n a la base de datos:\n",
    "\n",
    "La funci√≥n construye una URL de conexi√≥n a la base de datos usando las credenciales proporcionadas y establece una conexi√≥n utilizando SQLAlchemy (create_engine).\n",
    "\n",
    "Tambi√©n se usa el inspector de SQLAlchemy para obtener informaci√≥n sobre las tablas existentes en la base de datos.\n",
    "\n",
    "Recorrer las tablas en el orden de inserci√≥n:\n",
    "\n",
    "La funci√≥n recorre la lista insertion_order para insertar las tablas en el orden especificado.\n",
    "\n",
    "Verificaci√≥n de la existencia de tablas:\n",
    "\n",
    "Si la tabla ya existe en la base de datos, la funci√≥n imprime un mensaje indicando que la tabla existe.\n",
    "\n",
    "Si la tabla no existe, se imprimir√° un mensaje indicando que la tabla se crear√° autom√°ticamente.\n",
    "\n",
    "Eliminaci√≥n de duplicados (si se especifica unique_columns):\n",
    "\n",
    "Si se pasa el par√°metro unique_columns, la funci√≥n elimina las filas duplicadas de cada DataFrame bas√°ndose en las columnas definidas como \"√∫nicas\" en unique_columns.\n",
    "\n",
    "Este paso es importante porque solo se insertar√°n registros nuevos, evitando duplicados en la base de datos.\n",
    "\n",
    "Inserci√≥n de los registros:\n",
    "\n",
    "La funci√≥n recorre cada fila del DataFrame y genera una consulta INSERT INTO utilizando la cl√°usula ON CONFLICT DO NOTHING.\n",
    "\n",
    "Esto asegura que si ya existe un registro con las mismas columnas √∫nicas (unique_cols), el registro no se inserte nuevamente (evita duplicados).\n",
    "\n",
    "La consulta ON CONFLICT ({', '.join(unique_cols)}) DO NOTHING indica que, si ocurre un conflicto de clave (es decir, ya existe un registro con los mismos valores en las columnas indicadas en unique_cols), no se realiza ninguna acci√≥n (se ignora esa inserci√≥n).\n",
    "\n",
    "Manejo de errores:\n",
    "\n",
    "Si ocurre un error durante la inserci√≥n de alg√∫n registro (por ejemplo, un error SQL), la funci√≥n captura la excepci√≥n y muestra el mensaje de error correspondiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c660c0-552c-43aa-a798-ca4b1d353c10",
   "metadata": {},
   "source": [
    "### insert_dataframes_to_postgres\n",
    "\n",
    "üîç ¬øQu√© hace?\n",
    "\n",
    " Tiene como objetivo insertar datos de DataFrames de pandas en una base de datos PostgreSQL, evitando que se inserten registros duplicados basados en columnas √∫nicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "369ad419-9c42-43b1-abca-ae242bbab9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dataframes_to_postgres(dataframes, insertion_order, postgres_user, postgres_password, postgres_db, host='localhost', port=5432, if_exists='replace', unique_columns=None):\n",
    "    db_url = f\"postgresql+psycopg2://{postgres_user}:{postgres_password}@{host}:{port}/{postgres_db}\"\n",
    "    print(\"db_url\", db_url)\n",
    "    engine = create_engine(db_url)\n",
    "    \n",
    "    for table in insertion_order:\n",
    "        df = dataframes.get(table)\n",
    "        if df is None:\n",
    "            print(f\"‚ö†Ô∏è  No se encontr√≥ DataFrame para la tabla: {table}\")\n",
    "            continue\n",
    "        print(f\"üîÑ Insertando en tabla: {table}\")\n",
    "        \n",
    "        try:\n",
    "            if unique_columns and table in unique_columns:\n",
    "                unique_cols = unique_columns[table]\n",
    "                df = df.drop_duplicates(subset=unique_cols, keep='first')\n",
    "                print(f\"üîç Se eliminaron duplicados basados en las columnas: {unique_cols}\")\n",
    "            \n",
    "            # Usar to_sql para insertar el DataFrame\n",
    "            df.to_sql(table, engine, index=False, if_exists=if_exists)\n",
    "            print(f\"‚úÖ Datos insertados correctamente en la tabla '{table}'\")\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"‚ùå Error al insertar '{table}': {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error inesperado al insertar '{table}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c594a3-6e0c-4a4d-aafa-2229ae572279",
   "metadata": {},
   "source": [
    "## Dibujar graficos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd4e55-5ad1-45a2-b9fe-743f3a6dd509",
   "metadata": {},
   "source": [
    "### draw_graf\n",
    "\n",
    "üîç ¬øQu√© hace?\n",
    "\n",
    "Es una funci√≥n para visualizar un grafo utilizando NetworkX para la estructura del grafo y Matplotlib para la visualizaci√≥n. \n",
    "\n",
    "üß± Explicaci√≥n paso a paso\n",
    "\n",
    "Los nodos est√°n posicionados de manera autom√°tica utilizando el algoritmo de disposici√≥n de primavera.\n",
    "\n",
    "Los nodos tienen un tama√±o y color definidos.\n",
    "\n",
    "Las etiquetas de los nodos muestran un atributo personalizado ('columnas_comunes').\n",
    "\n",
    "Las aristas son flechas (porque el grafo es dirigido).\n",
    "\n",
    "El grafo tiene un t√≠tulo que es pasado como par√°metro.\n",
    "\n",
    "Es ideal para representar estructuras de datos, redes de relaciones o dependencias entre elementos (como en an√°lisis de grafos de dependencias o redes de informaci√≥n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd48a0f7-dc69-4716-a457-d9c0d4eb43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graf(grafo, titulo, color):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = nx.spring_layout(grafo, k=2)\n",
    "    nx.draw(grafo, pos, with_labels=True, node_size=5000, node_color=color, font_size=10, font_weight='bold', arrows=True)\n",
    "    labels = nx.get_node_attributes(grafo, 'columnas_comunes')\n",
    "    nx.draw_networkx_labels(grafo, pos, labels=labels, font_size=8, font_color='black')\n",
    "    plt.title(titulo)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef7d23-acc5-4d63-8426-588dcf48d834",
   "metadata": {},
   "source": [
    "### dependency_dataframes\n",
    "\n",
    "üîç ¬øQu√© hace?\n",
    "\n",
    "Analiza los DataFrames, crea grafos de dependencias y los visualiza.\n",
    "\n",
    "    Args:\n",
    "    \n",
    " üîπ globals_dict: Diccionario con variables globales (como globals()).\n",
    " \n",
    " üîπ split_key: Subcadena para separar DataFrames en dos grupos.\n",
    "\n",
    "üß± Explicaci√≥n paso a paso\n",
    "\n",
    "üîπ Clasifica las relaciones seg√∫n si uno de los DataFrames contiene un prefijo como \"df_M4\".\n",
    "\n",
    "üîπ Dibuja un grafo visual para mostrar c√≥mo est√°n conectados.\n",
    "\n",
    "üîπ Devuelve toda esa informaci√≥n para poder usarla luego.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac972d3e-e7bb-47f6-8e0a-80f3849af0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_dataframes(globals_dict, split_key=\"df_M4\"):\n",
    "    \n",
    "\n",
    "    relaciones = {}\n",
    "    relaciones_M4 = {}\n",
    "    relaciones_NO_M4 = {}\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    G_NO_M4 = nx.DiGraph()\n",
    "    G_M4 = nx.DiGraph()\n",
    "\n",
    "    df_names = [name for name in globals_dict if name.startswith('df_') and isinstance(globals_dict.get(name), pd.DataFrame)]\n",
    "\n",
    "    if not df_names:\n",
    "        return {}, {}, {}, G, G_M4, G_NO_M4\n",
    "\n",
    "    for dfx_name in df_names:\n",
    "        try:\n",
    "            df1 = globals_dict[dfx_name]\n",
    "            columnas_df1 = list(df1.columns)\n",
    "\n",
    "            for df2_name in df_names:\n",
    "                if df2_name != dfx_name:\n",
    "                    df2 = globals_dict[df2_name]\n",
    "                    columnas_df2 = list(df2.columns)\n",
    "                    columnas_comunes = list(set(columnas_df1) & set(columnas_df2))\n",
    "\n",
    "                    if columnas_comunes:\n",
    "                        if df2_name.startswith(split_key) or dfx_name.startswith(split_key):\n",
    "                            relaciones_M4[f\"{dfx_name} - {df2_name}\"] = columnas_comunes\n",
    "                            G_M4.add_edge(dfx_name, df2_name)\n",
    "                        else:\n",
    "                            relaciones_NO_M4[f\"{dfx_name} - {df2_name}\"] = columnas_comunes\n",
    "                            G_NO_M4.add_edge(dfx_name, df2_name)\n",
    "\n",
    "                        G.add_edge(dfx_name, df2_name)\n",
    "                        relaciones[f\"{dfx_name} - {df2_name}\"] = columnas_comunes\n",
    "        except KeyError as e:\n",
    "            print(f\"Error al acceder al DataFrame '{e}': Verifica que el DataFrame est√© definido.\")\n",
    "            continue\n",
    "\n",
    "    # Dibujar grafos\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    pos = nx.spring_layout(G, k=0.8)\n",
    "\n",
    "    nx.draw(G_M4, pos, with_labels=True, node_size=4000, node_color='lightblue',\n",
    "            font_size=10, font_weight='bold', edge_color='blue', arrows=True)\n",
    "    nx.draw(G_NO_M4, pos, with_labels=True, node_size=4000, node_color='green',\n",
    "            font_size=10, font_weight='bold', edge_color='lightgreen', arrows=True)\n",
    "\n",
    "    plt.title(\"Grafo de Dependencias entre DataFrames\")\n",
    "    plt.show()\n",
    "\n",
    "    # Devolver resultados\n",
    "    return relaciones, relaciones_M4, relaciones_NO_M4, G, G_M4, G_NO_M4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71a261-0d78-4efb-afa3-de8cfda62256",
   "metadata": {},
   "source": [
    "### tree_dataframes\n",
    "\n",
    "üîç ¬øQu√© hace?\n",
    "\n",
    "Analiza los DataFrames, crea un gr√°fico de arborescencia de dependencias y lo visualiza.\n",
    "\n",
    "Args:\n",
    "\n",
    "üîπglobals_dict: Diccionario con variables globales (como globals()).\n",
    "        \n",
    "üîπsplit_key: Subcadena para separar DataFrames en dos grupos.\n",
    "\n",
    "üß± ¬øQu√© hace paso a paso?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ca7f5-968f-482d-b107-0c17b3d2e941",
   "metadata": {},
   "source": [
    "## Funciones GPS\n",
    "\n",
    "Aqui agrupamos las funciones de apoyo para temas de localizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "801f3881-6280-4e87-9df4-ea74b54a7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tree_dataframes(globals_dict, split_key=\"df_M4\"):\n",
    "    relaciones = {}\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Detectar los DataFrames\n",
    "    df_names = [\n",
    "        name for name in globals_dict\n",
    "        if name.startswith('df_') and isinstance(globals_dict[name], pd.DataFrame)\n",
    "    ]\n",
    "\n",
    "    if not df_names:\n",
    "        print(\"No se encontraron DataFrames.\")\n",
    "        return\n",
    "\n",
    "    # Buscar relaciones reales basadas en columnas comunes\n",
    "    for dfx_name in df_names:\n",
    "        df1 = globals_dict[dfx_name]\n",
    "        columnas_df1 = set(df1.columns)\n",
    "\n",
    "        for df2_name in df_names:\n",
    "            if df2_name != dfx_name:\n",
    "                df2 = globals_dict[df2_name]\n",
    "                columnas_df2 = set(df2.columns)\n",
    "                columnas_comunes = columnas_df1 & columnas_df2\n",
    "\n",
    "                if columnas_comunes:\n",
    "                    G.add_edge(dfx_name, df2_name)\n",
    "                    relaciones[f\"{dfx_name} -> {df2_name}\"] = list(columnas_comunes)\n",
    "\n",
    "    # Eliminar ciclos detectados para evitar dependencias circulares\n",
    "    try:\n",
    "        ciclo = list(nx.simple_cycles(G))\n",
    "        if ciclo:\n",
    "            print(f\"Ciclos detectados en el grafo: {ciclo}\")\n",
    "            G.remove_edges_from([(n1, n2) for n1, n2 in G.edges() if (n1, n2) in ciclo])\n",
    "    except nx.NetworkXError as e:\n",
    "        print(f\"Error al eliminar ciclos: {e}\")\n",
    "    \n",
    "    # Encontrar nodos ra√≠z (sin predecesores)\n",
    "    posibles_raices = [n for n in G.nodes if G.in_degree(n) == 0]\n",
    "\n",
    "    if not posibles_raices:\n",
    "        print(\"No se encontr√≥ un DataFrame ra√≠z. El grafo tiene ciclos o todos tienen predecesores.\")\n",
    "        return\n",
    "\n",
    "    # Si hay m√°s de una ra√≠z, tratamos de crear √°rboles por separado para cada una\n",
    "    arborescencias = {}\n",
    "    for root_df in posibles_raices:\n",
    "        try:\n",
    "            T = nx.bfs_tree(G, root_df)\n",
    "            arborescencias[root_df] = T\n",
    "        except nx.NetworkXError as e:\n",
    "            print(f\"Error al construir el √°rbol desde {root_df}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Mostrar relaciones\n",
    "    for rel, cols in relaciones.items():\n",
    "        print(f\"{rel}: columnas comunes -> {cols}\")\n",
    "\n",
    "    # Dibujar todas las arborescencias encontradas\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    pos = nx.spring_layout(G, k=0.8)\n",
    "\n",
    "    # Dibujar cada √°rbol ra√≠z\n",
    "    for root_df, T in arborescencias.items():\n",
    "        nx.draw(T, pos, with_labels=True, node_size=4000, node_color='lightblue',\n",
    "                font_size=10, font_weight='bold', edge_color='blue', arrows=True)\n",
    "    \n",
    "    plt.title(\"Arborescencia de Dependencias entre DataFrames\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc510d55-74ba-4294-83b9-e349b565a74e",
   "metadata": {},
   "source": [
    "### haversine\n",
    "üîç ¬øQu√© hace?\n",
    "\n",
    "Calcula la distancia entre dos puntos en la superficie de la Tierra utilizando la f√≥rmula de Haversine, que considera la curvatura del planeta (es decir, no asume una superficie plana como lo har√≠a una distancia euclidiana).\n",
    "\n",
    "Par√°metros:\n",
    "\n",
    "    üîπlat1, lon1: Latitud y longitud del primer punto (en grados decimales).\n",
    "\n",
    "    üîπlat2, lon2: Latitud y longitud del segundo punto (tambi√©n en grados decimales).\n",
    "\n",
    "üß† Explicaci√≥n paso a paso:\n",
    "\n",
    "    üîπ Radio de la Tierra (R): Se toma como 6371 km (valor promedio).\n",
    "\n",
    "    üîπ Conversi√≥n a radianes: Las funciones trigonom√©tricas de math usan radianes, as√≠ que se convierte cada latitud y longitud de grados a radianes.\n",
    "\n",
    "    üîπ Diferencia angular: Calcula la diferencia entre latitudes y longitudes en radianes (dlat, dlon).\n",
    "\n",
    "üéº F√≥rmula de Haversine:\n",
    "\n",
    "    üîπ Calcula el valor de a, que es una medida intermedia basada en el seno de las mitades de las diferencias angulares.\n",
    "\n",
    "    üîπ Luego obtiene c, que es el √°ngulo central entre los dos puntos en una esfera.\n",
    "\n",
    "    üîπ Distancia final: Multiplica el radio de la Tierra por c para obtener la distancia en kil√≥metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51393513-3e34-4167-b020-b3d2021ddc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radio de la Tierra en kil√≥metros\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convertir de grados a radianes\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "    \n",
    "    # Diferencias de latitudes y longitudes\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # F√≥rmula de Haversine\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # Distancia en kil√≥metros\n",
    "    distance = R * c\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293253cf-70f2-4394-bee9-9b148209d370",
   "metadata": {},
   "source": [
    "### location_gps\n",
    "üîç ¬øQu√© hace?\n",
    "\n",
    "Intenta determinar la ubicaci√≥n del dispositivo bas√°ndose en la IP.\n",
    "\n",
    "üß† ¬øQu√© hace paso a paso?\n",
    "\n",
    "Llama a geocoder.ip('me'), que intenta determinar la ubicaci√≥n del dispositivo bas√°ndose en la IP.\n",
    "\n",
    "Extrae las coordenadas de latitud y longitud (latlng) del resultado.\n",
    "\n",
    "Muestra las coordenadas por consola con print()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2b6f7d8-5b24-4be6-b39b-1eb4e8186457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_gps():\n",
    "    ubicacion = geocoder.ip('me')\n",
    "    \n",
    "    # Mostrar coordenadas\n",
    "    print(f\"Tu ubicaci√≥n aproximada es: {ubicacion.latlng}\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
